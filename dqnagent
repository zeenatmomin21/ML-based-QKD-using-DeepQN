import pennylane as qml
from pennylane import numpy as np
import autograd.numpy as anp # Import autograd's numpy

# A small number of episodes to keep the training time short for this example.
num_episodes = 20
learning_rate = 0.01

# --- 1. BB84 Protocol Simulation Environment ---
class BB84Environment:
    """
    A simplified environment for the BB84 protocol.

    The state is a qubit sent by Alice. The action is Bob's choice of basis.
    The reward is based on whether Bob's measurement matches Alice's original bit.
    """
    def __init__(self):
        # The quantum device used for the simulation
        self.dev = qml.device("default.qubit", wires=1)

    def step(self, alice_bit, alice_basis, bob_basis):
        """
        Simulates one round of the protocol.

        Args:
            alice_bit (int): The bit Alice wants to send (0 or 1).
            alice_basis (int): Alice's chosen basis (0 for Z, 1 for X).
            bob_basis (int): Bob's chosen basis (0 for Z, 1 for X).

        Returns:
            tuple: (reward, done, bob_bit)
        """
        # Quantum circuit function
        @qml.qnode(self.dev)
        def bb84_circuit():
            # Step 1: Alice's preparation
            if alice_bit == 1:
                qml.PauliX(wires=0)
            if alice_basis == 1:  # Diagonal basis
                qml.Hadamard(wires=0)

            # Step 2: Bob's measurement
            if bob_basis == 1:  # Diagonal basis
                qml.Hadamard(wires=0)
            return qml.expval(qml.PauliZ(wires=0))

        # Execute the circuit and get the expectation value.
        # Note: We are not strictly simulating a single shot measurement here,
        # but rather the expected outcome for simplicity in this RL example.
        measurement = bb84_circuit()


        # Convert the expectation value to a bit
        # A positive expectation value is closer to the |0> state (bit 0)
        # A negative expectation value is closer to the |1> state (bit 1)
        bob_bit = 1 if measurement < 0 else 0

        # Calculate the reward
        reward = 0
        if alice_basis == bob_basis:
            # Reward is 1 if the bases match and the bit is correctly received
            if bob_bit == alice_bit:
                reward = 1
            else:
                # A penalty for incorrect bit reading on matching bases
                # This encourages the agent to learn to receive the correct bit
                reward = -1
        else:
            # Reward is 0 if bases don't match, as no useful key is generated
            # In a real BB84, these rounds are discarded. Here, they don't
            # contribute positively or negatively to the learning signal directly
            # based on bit value, only on the basis choice implicitly.
            reward = 0


        # The episode is always "done" after a single step in this simplified model
        done = True

        return reward, done, bob_bit

# --- 2. The RL Agent (Bob) using a Quantum Neural Network (QNN) ---
def quantum_agent(weights):
    """
    A simple QNN to act as the RL agent.

    This agent receives the classical bit from Alice as input and
    outputs a probability distribution for its basis choice.

    Args:
        weights (np.array): The trainable parameters of the QNN.

    Returns:
        float: The expectation value, which we'll map to a probability.
    """
    # The agent's quantum circuit to decide the basis
    # This circuit takes no quantum input, its input is the classical bit from Alice,
    # but we'll feed that classically to the agent's decision-making around the QNN.
    # The QNN itself just has trainable parameters.
    qml.RY(weights[0], wires=0)
    qml.RZ(weights[1], wires=0)
    return qml.expval(qml.PauliZ(0))


# PennyLane setup for training the agent
dev = qml.device("default.qubit", wires=1)
# Specify the autograd interface here
qnode = qml.QNode(quantum_agent, dev, interface="autograd")


# We use an Adam optimizer for training
optimizer = qml.AdamOptimizer(stepsize=learning_rate)

# Initial random weights for the QNN
weights = np.random.uniform(0, 2 * np.pi, 2, requires_grad=True)

# --- 3. Training Loop ---
env = BB84Environment()
episode_rewards = []

print("Starting training...")
for episode in range(num_episodes):
    total_reward = 0

    # Alice randomly generates a bit and a basis for the episode
    alice_bit = np.random.randint(0, 2)
    alice_basis = np.random.randint(0, 2)

    # The agent uses its QNN to decide on a basis
    qnn_output = qnode(weights)

    # Map the output to a probability using a sigmoid function
    prob_bob_basis_1 = 1 / (np.exp(-qnn_output) + 1) # Use np from pennylane for this calculation

    # Bob's action (basis choice) is sampled from the probability distribution
    bob_basis = 1 if np.random.rand() < prob_bob_basis_1 else 0

    # Simulate one step of the protocol
    reward, done, _ = env.step(alice_bit, alice_basis, bob_basis)
    total_reward += reward

    # Define the loss function for the QNN agent
    # We want to maximize reward, so we minimize -reward.
    # The loss is based on the reward received and the probability of the action taken.
    # This is a basic policy gradient approach.
    def loss_fn(weights):
        qnn_output_loss = qnode(weights)
        # Calculate probability using autograd's numpy for differentiation compatibility
        prob_bob_basis_1_loss = 1 / (anp.exp(-qnn_output_loss) + 1)

        if bob_basis == 1:
            # If Bob chose basis 1, we want to increase the probability of choosing basis 1
            # if the reward was positive, and decrease it if negative.
            return -reward * anp.log(prob_bob_basis_1_loss)
        else:
            # If Bob chose basis 0, we want to increase the probability of choosing basis 0
            # (i.e., decrease the probability of choosing basis 1) if the reward was positive,
            # and vice versa if negative.
            return -reward * anp.log(1 - prob_bob_basis_1_loss)


    # Update the QNN weights using the optimizer
    weights, _ = optimizer.step_and_cost(loss_fn, weights)


    episode_rewards.append(total_reward)

    print(f"Episode {episode+1}/{num_episodes}, Reward: {total_reward}")

print("\nTraining finished.")
print("Final QNN weights:", weights)

# --- 4. Testing the trained model ---
print("\nTesting the trained model...")
test_matches = 0
num_tests = 50
for _ in range(num_tests):
    alice_bit = np.random.randint(0, 2)
    alice_basis = np.random.randint(0, 2)

    # For testing, we use the deterministic action based on the learned probability
    qnn_output = qnode(weights)
    prob_bob_basis_1 = 1 / (np.exp(-qnn_output) + 1) # Use np from pennylane for this calculation
    bob_basis = 1 if prob_bob_basis_1 > 0.5 else 0 # Use the deterministic choice for testing


    _, _, bob_bit = env.step(alice_bit, alice_basis, bob_basis)

    # We are interested in the bits that match when bases are the same
    if alice_basis == bob_basis:
        if alice_bit == bob_bit:
            test_matches += 1

print(f"Test results: {test_matches} successful key bits out of {num_tests} rounds where bases matched.")

